version: '3'
services:
  tgi_server:
    image: me020523/text-generation-inference:1.0.1
    container_name: tgi-server
    restart: unless-stopped
    volumes:
      - ${HOME}/.cache/huggingface/hub:/data
    ports:
      - 8080:80
    env_file:
      - .env
    command: ["--model-id", "${LLM_MODEL}","--json-output", "--quantize", "gptq", "--max-total-tokens", "4096", "--max-input-length", "2048", "--max-batch-prefill-tokens", "2048"]
    shm_size: "1G"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  tgi_openai:
    build: .
    container_name: tgi-openai
    restart: unless-stopped
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    ports:
      - 8000:8000
    env_file:
      - .env
    depends_on:
      - tgi_server
    shm_size: "1G"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
