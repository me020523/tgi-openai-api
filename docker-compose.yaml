version: '3'
services:
  tgi_server:
    image: ghcr.io/huggingface/text-generation-inference:1.0.0
    container_name: tgi-server
    restart: unless-stopped
    volumes:
      - ${HOME}/.cache/huggingface/hub:/data
    ports:
      - 8081:8080
    env_file:
      - .env
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ['text-generation-launcher', "--model-id", "${LLM_MODEL}","--json-output", "--quantize", "gptq", "--max-total-tokens", "4096", "--max-input-length", "2048", "--max-batch-prefill-tokens", "2048"]
    shm_size: "1G"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  tgi_openai:
    build: .
    container_name: tgi-openai
    restart: unless-stopped
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    ports:
      - 8080:8080
    env_file:
      - .env
    depends_on:
      - tgi_server
    shm_size: "1G"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
